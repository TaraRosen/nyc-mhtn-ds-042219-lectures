{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /anaconda3/lib/python3.7/site-packages (0.90)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.7/site-packages (from xgboost) (1.2.1)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from xgboost) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784753\n",
      "F1: 0.675676\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 2, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.640294</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.640921</td>\n",
       "      <td>0.011085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629094</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.630053</td>\n",
       "      <td>0.011036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.617743</td>\n",
       "      <td>0.016020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.601325</td>\n",
       "      <td>0.011162</td>\n",
       "      <td>0.602671</td>\n",
       "      <td>0.015340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.590203</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.592608</td>\n",
       "      <td>0.015515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.580331</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.583432</td>\n",
       "      <td>0.011188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.570725</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.574913</td>\n",
       "      <td>0.009321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.564326</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.569265</td>\n",
       "      <td>0.009131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.557697</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.563021</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.552730</td>\n",
       "      <td>0.015043</td>\n",
       "      <td>0.558481</td>\n",
       "      <td>0.007250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.547068</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>0.552817</td>\n",
       "      <td>0.008987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.542089</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.548452</td>\n",
       "      <td>0.011089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.536571</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.543687</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.531027</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>0.538226</td>\n",
       "      <td>0.012720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.528377</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.535746</td>\n",
       "      <td>0.012291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.523079</td>\n",
       "      <td>0.008863</td>\n",
       "      <td>0.531756</td>\n",
       "      <td>0.013968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.518823</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.527936</td>\n",
       "      <td>0.016220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.516713</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.525689</td>\n",
       "      <td>0.017301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.512552</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.521564</td>\n",
       "      <td>0.015942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.518616</td>\n",
       "      <td>0.013734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.503175</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.513420</td>\n",
       "      <td>0.017039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.500755</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.511212</td>\n",
       "      <td>0.017609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.498579</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.509316</td>\n",
       "      <td>0.019612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.497073</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.508617</td>\n",
       "      <td>0.020337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.493866</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.505923</td>\n",
       "      <td>0.018245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.492352</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.505034</td>\n",
       "      <td>0.018548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.488268</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.501230</td>\n",
       "      <td>0.021190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.485094</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.498670</td>\n",
       "      <td>0.023099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.483280</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.024599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.405556</td>\n",
       "      <td>0.010567</td>\n",
       "      <td>0.438868</td>\n",
       "      <td>0.033639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.405416</td>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.438799</td>\n",
       "      <td>0.033762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.405035</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.438441</td>\n",
       "      <td>0.034176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.404769</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>0.438322</td>\n",
       "      <td>0.034408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.404579</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.438402</td>\n",
       "      <td>0.034433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.404267</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.438157</td>\n",
       "      <td>0.034757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.403527</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.437963</td>\n",
       "      <td>0.034813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.403150</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.437628</td>\n",
       "      <td>0.034788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.402852</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.437338</td>\n",
       "      <td>0.035082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.402380</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.437535</td>\n",
       "      <td>0.035311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.402135</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>0.437321</td>\n",
       "      <td>0.035330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.401965</td>\n",
       "      <td>0.010690</td>\n",
       "      <td>0.437123</td>\n",
       "      <td>0.035649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.401602</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.437043</td>\n",
       "      <td>0.036012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.401007</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.035821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.400863</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.436732</td>\n",
       "      <td>0.035924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.400192</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.436602</td>\n",
       "      <td>0.036186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.399852</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>0.436334</td>\n",
       "      <td>0.035925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.399639</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.436299</td>\n",
       "      <td>0.035879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.399345</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.436111</td>\n",
       "      <td>0.035887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.399158</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>0.436208</td>\n",
       "      <td>0.036155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.398955</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.436230</td>\n",
       "      <td>0.036228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.398512</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.436171</td>\n",
       "      <td>0.036488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.398139</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.436183</td>\n",
       "      <td>0.036983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.397733</td>\n",
       "      <td>0.011659</td>\n",
       "      <td>0.435879</td>\n",
       "      <td>0.037050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.397330</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.435761</td>\n",
       "      <td>0.037144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.396885</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.037383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.396632</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.435757</td>\n",
       "      <td>0.037410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.396027</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.037449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.395488</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>0.435333</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.394918</td>\n",
       "      <td>0.011901</td>\n",
       "      <td>0.435028</td>\n",
       "      <td>0.037540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.640294           0.007082           0.640921   \n",
       "2              0.629094           0.006565           0.630053   \n",
       "3              0.616500           0.011333           0.617743   \n",
       "4              0.601325           0.011162           0.602671   \n",
       "5              0.590203           0.014328           0.592608   \n",
       "6              0.580331           0.011428           0.583432   \n",
       "7              0.570725           0.015010           0.574913   \n",
       "8              0.564326           0.016661           0.569265   \n",
       "9              0.557697           0.015285           0.563021   \n",
       "10             0.552730           0.015043           0.558481   \n",
       "11             0.547068           0.015693           0.552817   \n",
       "12             0.542089           0.015239           0.548452   \n",
       "13             0.536571           0.017683           0.543687   \n",
       "14             0.531027           0.013537           0.538226   \n",
       "15             0.528377           0.012291           0.535746   \n",
       "16             0.523079           0.008863           0.531756   \n",
       "17             0.518823           0.006483           0.527936   \n",
       "18             0.516713           0.006924           0.525689   \n",
       "19             0.512552           0.004859           0.521564   \n",
       "20             0.508500           0.005185           0.518616   \n",
       "21             0.503175           0.002684           0.513420   \n",
       "22             0.500755           0.003567           0.511212   \n",
       "23             0.498579           0.004905           0.509316   \n",
       "24             0.497073           0.005650           0.508617   \n",
       "25             0.493866           0.006596           0.505923   \n",
       "26             0.492352           0.006470           0.505034   \n",
       "27             0.488268           0.004673           0.501230   \n",
       "28             0.485094           0.007452           0.498670   \n",
       "29             0.483280           0.008005           0.497143   \n",
       "..                  ...                ...                ...   \n",
       "100            0.405556           0.010567           0.438868   \n",
       "101            0.405416           0.010647           0.438799   \n",
       "102            0.405035           0.010488           0.438441   \n",
       "103            0.404769           0.010535           0.438322   \n",
       "104            0.404579           0.010568           0.438402   \n",
       "105            0.404267           0.010597           0.438157   \n",
       "106            0.403527           0.010805           0.437963   \n",
       "107            0.403150           0.010858           0.437628   \n",
       "108            0.402852           0.010817           0.437338   \n",
       "109            0.402380           0.010784           0.437535   \n",
       "110            0.402135           0.010734           0.437321   \n",
       "111            0.401965           0.010690           0.437123   \n",
       "112            0.401602           0.010874           0.437043   \n",
       "113            0.401007           0.011147           0.436706   \n",
       "114            0.400863           0.011151           0.436732   \n",
       "115            0.400192           0.011175           0.436602   \n",
       "116            0.399852           0.011498           0.436334   \n",
       "117            0.399639           0.011438           0.436299   \n",
       "118            0.399345           0.011369           0.436111   \n",
       "119            0.399158           0.011383           0.436208   \n",
       "120            0.398955           0.011392           0.436230   \n",
       "121            0.398512           0.011461           0.436171   \n",
       "122            0.398139           0.011486           0.436183   \n",
       "123            0.397733           0.011659           0.435879   \n",
       "124            0.397330           0.011819           0.435761   \n",
       "125            0.396885           0.011926           0.435700   \n",
       "126            0.396632           0.011839           0.435757   \n",
       "127            0.396027           0.011676           0.435443   \n",
       "128            0.395488           0.011790           0.435333   \n",
       "129            0.394918           0.011901           0.435028   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.011085  \n",
       "2            0.011036  \n",
       "3            0.016020  \n",
       "4            0.015340  \n",
       "5            0.015515  \n",
       "6            0.011188  \n",
       "7            0.009321  \n",
       "8            0.009131  \n",
       "9            0.007609  \n",
       "10           0.007250  \n",
       "11           0.008987  \n",
       "12           0.011089  \n",
       "13           0.011051  \n",
       "14           0.012720  \n",
       "15           0.012291  \n",
       "16           0.013968  \n",
       "17           0.016220  \n",
       "18           0.017301  \n",
       "19           0.015942  \n",
       "20           0.013734  \n",
       "21           0.017039  \n",
       "22           0.017609  \n",
       "23           0.019612  \n",
       "24           0.020337  \n",
       "25           0.018245  \n",
       "26           0.018548  \n",
       "27           0.021190  \n",
       "28           0.023099  \n",
       "29           0.024599  \n",
       "..                ...  \n",
       "100          0.033639  \n",
       "101          0.033762  \n",
       "102          0.034176  \n",
       "103          0.034408  \n",
       "104          0.034433  \n",
       "105          0.034757  \n",
       "106          0.034813  \n",
       "107          0.034788  \n",
       "108          0.035082  \n",
       "109          0.035311  \n",
       "110          0.035330  \n",
       "111          0.035649  \n",
       "112          0.036012  \n",
       "113          0.035821  \n",
       "114          0.035924  \n",
       "115          0.036186  \n",
       "116          0.035925  \n",
       "117          0.035879  \n",
       "118          0.035887  \n",
       "119          0.036155  \n",
       "120          0.036228  \n",
       "121          0.036488  \n",
       "122          0.036983  \n",
       "123          0.037050  \n",
       "124          0.037144  \n",
       "125          0.037383  \n",
       "126          0.037410  \n",
       "127          0.037449  \n",
       "128          0.037500  \n",
       "129          0.037540  \n",
       "\n",
       "[130 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup our parameters for XGBoost to test\n",
    "param_test1 = {\n",
    " 'max_depth':range(2,9,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
    "    param_grid = param_test1, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=140, n_jobs=1,\n",
       "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid={'max_depth': range(2, 9, 2), 'min_child_weight': range(1, 6, 2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.09179525, 0.09063001, 0.08777585, 0.12223573, 0.12836318,\n",
       "        0.12030063, 0.15372648, 0.16542702, 0.1569459 , 0.30257945,\n",
       "        0.20234499, 0.12792764]),\n",
       " 'std_fit_time': array([0.00609177, 0.00676233, 0.00401119, 0.00553185, 0.00440376,\n",
       "        0.00441763, 0.00571262, 0.00610245, 0.0334703 , 0.03376342,\n",
       "        0.03702872, 0.0094853 ]),\n",
       " 'mean_score_time': array([0.00452805, 0.00386176, 0.00484114, 0.0065599 , 0.00618863,\n",
       "        0.0044528 , 0.00497093, 0.00465422, 0.00452623, 0.00584483,\n",
       "        0.00489259, 0.00405927]),\n",
       " 'std_score_time': array([0.00049334, 0.00051341, 0.00111433, 0.00335621, 0.0020929 ,\n",
       "        0.0002766 , 0.00017284, 0.00022254, 0.00017796, 0.00071233,\n",
       "        0.00065397, 0.00053576]),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2, 'min_child_weight': 1},\n",
       "  {'max_depth': 2, 'min_child_weight': 3},\n",
       "  {'max_depth': 2, 'min_child_weight': 5},\n",
       "  {'max_depth': 4, 'min_child_weight': 1},\n",
       "  {'max_depth': 4, 'min_child_weight': 3},\n",
       "  {'max_depth': 4, 'min_child_weight': 5},\n",
       "  {'max_depth': 6, 'min_child_weight': 1},\n",
       "  {'max_depth': 6, 'min_child_weight': 3},\n",
       "  {'max_depth': 6, 'min_child_weight': 5},\n",
       "  {'max_depth': 8, 'min_child_weight': 1},\n",
       "  {'max_depth': 8, 'min_child_weight': 3},\n",
       "  {'max_depth': 8, 'min_child_weight': 5}],\n",
       " 'split0_test_score': array([0.90752388, 0.91206615, 0.91358025, 0.91847193, 0.9245283 ,\n",
       "        0.91963662, 0.91754018, 0.92103424, 0.9226648 , 0.91404612,\n",
       "        0.91800606, 0.9245283 ]),\n",
       " 'split1_test_score': array([0.83356627, 0.83962264, 0.83880736, 0.84195201, 0.84894014,\n",
       "        0.84369904, 0.82797577, 0.85406476, 0.84602842, 0.83519683,\n",
       "        0.8509201 , 0.84882367]),\n",
       " 'split2_test_score': array([0.84265083, 0.84078733, 0.83706033, 0.85371535, 0.84532961,\n",
       "        0.83484743, 0.85790822, 0.84649429, 0.83391568, 0.86559515,\n",
       "        0.84649429, 0.84020498]),\n",
       " 'split3_test_score': array([0.90865385, 0.90276442, 0.90877404, 0.89519231, 0.89615385,\n",
       "        0.90252404, 0.88822115, 0.90336538, 0.90492788, 0.89110577,\n",
       "        0.89711538, 0.90492788]),\n",
       " 'split4_test_score': array([0.88545673, 0.88401442, 0.89567308, 0.88774038, 0.89675481,\n",
       "        0.89519231, 0.89831731, 0.90132212, 0.89879808, 0.90144231,\n",
       "        0.90132212, 0.89951923]),\n",
       " 'mean_test_score': array([0.87557031, 0.87585099, 0.87877901, 0.8794144 , 0.88234134,\n",
       "        0.87917989, 0.87799253, 0.88525616, 0.88126697, 0.88147724,\n",
       "        0.88277159, 0.88360081]),\n",
       " 'std_test_score': array([0.03181611, 0.03047775, 0.03386567, 0.02795577, 0.03054106,\n",
       "        0.0336522 , 0.03158099, 0.02946699, 0.03482775, 0.02809328,\n",
       "        0.028712  , 0.03309365]),\n",
       " 'rank_test_score': array([12, 11,  9,  7,  4,  8, 10,  1,  6,  5,  3,  2], dtype=int32),\n",
       " 'split0_train_score': array([0.92888938, 0.91864833, 0.91423395, 0.98075274, 0.96360544,\n",
       "        0.94910529, 0.99514197, 0.97763975, 0.95669181, 0.99728631,\n",
       "        0.97968057, 0.95636646]),\n",
       " 'split1_train_score': array([0.94444691, 0.93937445, 0.93413191, 0.98355516, 0.96878882,\n",
       "        0.95595238, 0.99532683, 0.97895593, 0.96170512, 0.99785566,\n",
       "        0.979895  , 0.96177167]),\n",
       " 'split2_train_score': array([0.94376664, 0.93603963, 0.93171399, 0.98192103, 0.96876664,\n",
       "        0.95449571, 0.99423248, 0.97697427, 0.96018929, 0.99708666,\n",
       "        0.97776545, 0.96052943]),\n",
       " 'split3_train_score': array([0.93221868, 0.92835972, 0.92000352, 0.9805878 , 0.9627676 ,\n",
       "        0.95108066, 0.99217936, 0.97544495, 0.95673705, 0.99512861,\n",
       "        0.97641336, 0.95667102]),\n",
       " 'split4_train_score': array([0.93686265, 0.92923276, 0.92254193, 0.98061714, 0.96448432,\n",
       "        0.95123472, 0.99278095, 0.97403636, 0.9586812 , 0.99608234,\n",
       "        0.97692691, 0.95826303]),\n",
       " 'mean_train_score': array([0.93723685, 0.93033098, 0.92452506, 0.98148677, 0.96568256,\n",
       "        0.95237375, 0.99393232, 0.97661025, 0.95880089, 0.99668792,\n",
       "        0.97813626, 0.95872032]),\n",
       " 'std_train_score': array([0.00615833, 0.00715471, 0.00740616, 0.0011463 , 0.00258486,\n",
       "        0.00248844, 0.0012567 , 0.00171333, 0.00195367, 0.00096735,\n",
       "        0.00141749, 0.00212288])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'min_child_weight': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8852561582809223"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.780269\n",
      "F1: 0.662069\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2314bd30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=3, missing=nan, n_estimators=140, n_jobs=1,\n",
      "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.8, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
